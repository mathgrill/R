### https://www.kaggle.com/c/nyc-taxi-trip-duration/data

# https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-classified

# https://stackoverflow.com/questions/33903883/r-extract-hours-from-time-in-factor-format
# https://stackoverflow.com/questions/18999710/creating-a-new-column-to-a-data-frame-using-a-formula-from-another-variable
# https://www.rdocumentation.org/packages/h2o/versions/3.8.3.3/topics/h2o.deeplearning
# https://stackoverflow.com/questions/3418128/how-to-convert-a-factor-to-an-integer-numeric-without-a-loss-of-information
# https://www.stat.berkeley.edu/~s133/dates.html
# https://stackoverflow.com/questions/32363998/function-to-calculate-geospatial-distance-between-two-points-lat-long-using-r
# http://www.statmethods.net/input/contents.html

# set working dir
setwd("/Volumes/WD-fat/R/NYCtrip")

library('dplyr') # data manipulation

train <- read.csv(file="train.csv",nrows=10000)

train$vendor_id <- as.factor(train$vendor_id)
train$store_and_fwd_flag <- as.factor(train$store_and_fwd_flag)

glimpse(train)

# Missing Values
sum(is.na(train))

# multiplot
source("http://peterhaschke.com/Code/multiplot.R")

# installing a pkg
install.packages("data.table")
# load it
library(data.table)



# Step 2: Separate the dataset to 80% for training and 20% for testing
library (caret)
set.seed(712)
inTrain<- createDataPartition(train$trip_duration, p=0.8, list=FALSE)
training<-train[inTrain,]
testing<-train[-inTrain,]

#store the datasets into .csv files
write.csv (training , file = "train-data.csv", row.names = FALSE) 
write.csv (testing , file = "test-data.csv", row.names = FALSE)
nrow(training)
nrow(testing)

# Step 3: Load the h2o package

library(h2o)
 
# start a local h2o cluster
local.h2o <- h2o.init(ip = "localhost", port = 54321, startH2O = TRUE, nthreads=-1)

# Load the training and testing datasets and convert the label to digit factor
training <- read.csv ("train-data.csv") 
testing  <- read.csv ("test-data.csv")

# pass dataframe from inside of the R environment to the H2O instance
trData<-as.h2o(training)
tsData<-as.h2o(testing)

# Step 4: Train the model

res.dl <- h2o.deeplearning(x = 6:10, y = 11, trData, activation = "Tanh", hidden=rep(160,5),epochs = 20)

# install package
install.packages("geosphere")
library(geosphere)
istm (c(lon1, lat1), c(lon2, lat2), fun = distHaversine)
library(dplyr)
# append the distance between the points to the training dataset
training <- training %>% mutate(CTD = distHaversine(cbind(pickup_longitude, pickup_latitude), cbind(dropoff_longitude, dropoff_latitude)))

# list columns
names(training)
# remove column 12
training <-training[,-c(12)]
# update csv files
write.csv(testing, file = "testing.csv", row.names=FALSE)
write.csv(training, file = "training.csv", row.names=FALSE)

# round distance CTD
testing$CTD <- round(testing$CTD, 0)
write.csv(testing, file = "testing.csv")
training$CTD <- round(training$CTD, 0)
write.csv(training, file = "training.csv")

res.dl <- h2o.deeplearning(x = c(6,7,8,9,10,12), y = 11, trData, activation = "Tanh", hidden=rep(160,5),epochs = 20)

test <- test %>% mutate(CTD = distHaversine(cbind(pickup_longitude, pickup_latitude), cbind(dropoff_longitude, dropoff_latitude)))
write.csv(test, file = "test.csv", row.names=FALSE)

# make predictions
pred.dl<-h2o.predict(object=res.dl, newdata=test_h2o)
summary(pred.dl)
df.test <- as.data.frame(pred.dl)
write.csv(df.test, file = "submission3.csv")

# new iteration take into account start of the trip and day of the week
training$day <- weekdays(as.Date(training$pickup_datetime))
training$hour <- format(strptime(training$pickup_datetime,"%Y-%m-%d %H:%M:%S"),'%H')

test$day <- weekdays(as.Date(test$pickup_datetime))
test$hour <- format(strptime(test$pickup_datetime,"%Y-%m-%d %H:%M:%S"),'%H')
head(training, n=10)
head(test, n=10)

training$hour <- as.numeric(training$hour)
training$day <- as.factor(training$day)

test$hour <- as.numeric(test$hour)
test$day <- as.factor(test$day)

# pass test data to h2o 
test_h2o<-as.h2o(test)

res.dl <- h2o.deeplearning(x = c(6,7,8,9,10,12,13,14), y = 11, trData, activation = "Tanh", hidden=rep(160,5),epochs = 20)

# Compute the root mean squared log error
# https://artax.karlin.mff.cuni.cz/r-help/library/Metrics/html/rmsle.html
install.packages("Metrics")
library(Metrics)
rmsle(actual, predicted)

# get model
res.dl <- h2o.deeplearning(x = c(6,7,8,9,10,12), y = 11, trData, activation = "Tanh", hidden=rep(160,5),epochs = 20)
# make predictions
pred.dl<-h2o.predict(object=res.dl, newdata=tsData)
df.test <- as.data.frame(pred.dl)
testactual <- testing$trip_duration
# replace negative predictions by 0
df.test$predict <- ifelse(df.test$predict < 0, 0, df.test$predict)
# round
df.test$predict <- round(df.test$predict, 0)
# Compute the root mean squared log error
rmsle(df.test, testactual)

library('lubridate')
# convert to datetime format?
train <- train %>%
  mutate(pickup_datetime = ymd_hms(pickup_datetime),
         dropoff_datetime = ymd_hms(dropoff_datetime),
         vendor_id = factor(vendor_id),
         passenger_count = factor(passenger_count))
         
# add some vars - airports and dist
library('geosphere') # geospatial locations

jfk_coord <- tibble(lon = -73.778889, lat = 40.639722)
la_guardia_coord <- tibble(lon = -73.872611, lat = 40.77725)

pick_coord <- train %>%
  select(pickup_longitude, pickup_latitude)
drop_coord <- train %>%
  select(dropoff_longitude, dropoff_latitude)
train$dist <- distCosine(pick_coord, drop_coord)

train$jfk_dist_pick <- distCosine(pick_coord, jfk_coord)
train$jfk_dist_drop <- distCosine(drop_coord, jfk_coord)
train$lg_dist_pick <- distCosine(pick_coord, la_guardia_coord)
train$lg_dist_drop <- distCosine(drop_coord, la_guardia_coord)

train <- train %>%
  mutate(speed = dist/trip_duration*3.6,
         date = date(pickup_datetime),
         month = month(pickup_datetime),
         wday = wday(pickup_datetime),
         hour = hour(pickup_datetime),
         jfk_trip = (jfk_dist_pick < 2e3) | (jfk_dist_drop < 2e3),
         lg_trip = (lg_dist_pick < 2e3) | (lg_dist_drop < 2e3)
         )
# convert some vars to factors
train <- train %>%
  mutate(
         month = factor(month),
         wday = factor(wday),
         vendor_id = factor(vendor_id),
         store_and_fwd_flag = factor(store_and_fwd_flag),
         pickup_datetime = factor(pickup_datetime),
         dropoff_datetime = factor(dropoff_datetime)         
         )
# STEP gmaps stuff
# http://nagraj.net/notes/calculating-geographic-distance-with-r/
# http://maps.googleapis.com/maps/api/distancematrix/json?origins=40.76793671,-73.98215485&destinations=40.76560211,-73.96463013&mode=driving&sensor=false

# load ggmap
library(ggmap)

# calculate distance between two place names
arena_dist <- mapdist(from = "Madison Square Garden New York, NY", to = "The Palace of Auburn Hills Auburn Hills, MI")

arena_dist <- mapdist(from = "40.76793671, -73.98215485", to = "40.76560211, -73.96463013")

# output distance in miles
arena_dist$miles

# output "distance" in minutes
arena_dist$minutes

# how to do thos?
gm <- train %>%
+   select(mapdist(from = cat(train$pickup_latitude,", ",train$pickup_longitude), to = cat(train$dropoff_latitude,", ",train$dropoff_longitude)))

# remove column "new"
train <- subset(train, select=-c(new))

# this works to get results from coordinates:
# https://stackoverflow.com/questions/37125950/mapdist-error-is-characterfrom
library(ggmap)
X <- read.table(header=TRUE, text="pickup                 dropoff 
40.77419,-73.872608    40.78055,-73.955042
40.7737,-73.870721     40.757007,-73.971953")

X <- as.data.frame(lapply(X, function(x) sapply(as.character(x), function(y) URLencode(y, T) ) ), stringsAsFactors = F)
rownames(X) <- NULL

res <- mapdist(from= X$pickup, 
    to = X$dropoff, 
    mode = "driving" ,
    output = "simple", messaging = FALSE, sensor = FALSE,
    language = "en-EN", override_limit = FALSE)
cbind(X, res)

# create field from concatenated fields using paste
train <- train %>%
  mutate(
         pickup = paste(pickup_latitude,",",pickup_longitude),
         dropoff = paste(dropoff_latitude,",",dropoff_longitude)         
         )
# create new dataframe from some columns
gg <- train[,c("pickup","dropoff")]

gg <- as.data.frame(lapply(gg, function(x) sapply(as.character(x), function(y) URLencode(y, T) ) ), stringsAsFactors = F)
rownames(gg) <- NULL

res <- mapdist(from= gg$pickup, 
     to = gg$dropoff, 
     mode = "driving" ,
     output = "simple", messaging = FALSE, sensor = FALSE,
     language = "en-EN", override_limit = FALSE)

# append the res data to train
train <- cbind(train, res)

# detach all pkg
rm(list = ls(all = TRUE))

# if the same fn name appears in multiple libraries
# To specify the package that you want to use, the syntax is:
chron::is.weekend()
tseries::is.weekend()
In other words, use packagename::functionname()

# read this
# https://stackoverflow.com/questions/7505547/detach-all-packages-while-working-in-r

# try random forests?
# http://trevorstephens.com/kaggle-titanic-tutorial/r-part-5-random-forests/

# last run
res.dl <- h2o.deeplearning(x = c(2,6,7,8,9,10,12), y = 11, trData, activation = "Tanh", hidden=rep(160,5),epochs = 20)
#  0.5342811

# RANDOM FORESTS
# detach conflicting pkg
detach("package:dplyr", unload=TRUE)
fit <- randomForest(trip_duration ~ CTD + vendor_id + pickup_latitude + pickup_longitude + dropoff_latitude + dropoff_longitude,
                      data=train, 
                      importance=TRUE, 
                      ntree=2000)

# Variable Engineering
train$pickup_wday <- as.factor(weekdays(as.Date(train$pickup_datetime)))
train$pickup_hour <- as.numeric(substr(train$pickup_datetime,12,13))
train$pickup_month <- as.factor(months(as.Date(train$pickup_datetime)))


